import great_expectations as gx
from pyspark.sql import functions as F
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, LongType
import traceback
import os
import json
import threading
import gc

# --- å…¨å±€æ’é˜Ÿæ§åˆ¶ ---
_SHARED_GX_CONTEXT = None
# ä½¿ç”¨é€’å½’é” (RLock) ç¡®ä¿çº¿ç¨‹å®‰å…¨
gx_lock = threading.RLock() 

def get_gx_context():
    """åˆå§‹åŒ–å¹¶å¤ç”¨ GX Ephemeral Context"""
    global _SHARED_GX_CONTEXT
    with gx_lock:
        if _SHARED_GX_CONTEXT is None:
            # ephemeral æ¨¡å¼ä¸å†™ç£ç›˜ï¼Œé€‚åˆæµå¼å¤„ç†
            _SHARED_GX_CONTEXT = gx.get_context(mode="ephemeral")
            print("âœ… Initialised new Ephemeral Context (Serial Queue Mode)")
        return _SHARED_GX_CONTEXT

def load_suite_if_not_exists(context, suite_name):
    """ä» Volumes å®‰å…¨åŠ è½½ Expectation Suite"""
    try:
        return context.suites.get(name=suite_name)
    except Exception:
        suite_path = f"/Volumes/traffic_dev_catalog/gx/gx_configs/expectations/{suite_name}.json"
        if os.path.exists(suite_path):
            with open(suite_path, "r") as f:
                suite_dict = json.load(f)
            
            # æ¸…ç† 1.x å…¼å®¹æ€§å­—æ®µ
            suite_dict.pop("name", None) 
            suite_dict.pop("data_context_id", None)
            expectations = suite_dict.get("expectations", [])
            
            new_suite = gx.ExpectationSuite(
                name=suite_name,
                expectations=expectations
            )
            return context.suites.add(new_suite)
        else:
            raise FileNotFoundError(f"Missing Suite JSON at: {suite_path}")

def validate_and_insert_process_batch(df, catalog, schema, batch_id, table_name): 
    """ä¸²è¡Œæ’é˜Ÿå¤„ç† GX éªŒè¯ä¸æ•°æ®æŒä¹…åŒ–"""
    
    # è·å–å½“å‰ DataFrame çš„ SparkSessionï¼Œé¿å… NameError: name 'spark' is not defined
    spark_internal = df.sparkSession

    # 1. å¿«é€Ÿæ£€æŸ¥ç©ºæ‰¹æ¬¡
    if df.limit(1).count() == 0:
        return

    # è·¯å¾„ä¸é…ç½®å®šä¹‰
    full_target_table = f"{catalog}.{schema}.{table_name}"
    quarantine_table = f"{catalog}.gx.data_quality_quarantine"
    temp_id_col = "_dq_batch_id"
    ds_name = f"ds_{table_name}_{batch_id}"
    val_def_name = f"val_{table_name}_{batch_id}"
    
    # 2. ä¸ºéªŒè¯å‡†å¤‡æ•°æ®ï¼ˆå¸¦å”¯ä¸€ IDï¼‰
    # å¦‚æœ Driver å†…å­˜æŒç»­æŠ¥é”™ Allocation Failureï¼Œè¯·ç§»é™¤åº•éƒ¨çš„ .persist()
    df_with_id = df.withColumn(temp_id_col, F.monotonically_increasing_id()).persist()
    
    result = None 
    
    # --- ä¸²è¡Œé”å¼€å§‹ (æ ¸å¿ƒæ’é˜ŸåŒº) ---
    with gx_lock:
        try:
            print(f"ğŸ”’ Batch {batch_id}: Processing {table_name} (Serial Lock)...", flush=True)
            context = get_gx_context()
            
            # æ¸…ç†å†å²æ®‹ç•™ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
            try: context.validation_definitions.delete(val_def_name)
            except: pass
            try: context.data_sources.delete(ds_name)
            except: pass

            # é…ç½® GX å¯¹è±¡
            datasource = context.data_sources.add_spark(name=ds_name)
            asset = datasource.add_dataframe_asset(name=f"asset_{batch_id}")
            batch_def = asset.add_batch_definition_whole_dataframe(name="batch_def")
            suite = load_suite_if_not_exists(context, f"{table_name}_suite")
            
            val_definition = context.validation_definitions.add(
                gx.ValidationDefinition(name=val_def_name, data=batch_def, suite=suite)
            )

            # è¿è¡ŒéªŒè¯
            print(f"ğŸš€ Batch {batch_id}: Running GX validation...", flush=True)
            result = val_definition.run(
                batch_parameters={"dataframe": df_with_id},
                result_format={
                    "result_format": "COMPLETE", 
                    "unexpected_index_column_names": [temp_id_col]
                }
            )
            
            # ã€ä¼˜åŒ–ã€‘ç«‹å³æ¸…ç† GX å†…å­˜å¼•ç”¨
            context.validation_definitions.delete(val_def_name)
            context.data_sources.delete(ds_name)
            del val_definition, asset, datasource
            
        except Exception as e:
            print(f"âŒ Batch {batch_id} GX Error: {str(e)}")
            # é™çº§å¤„ç†ï¼šéªŒè¯æŠ¥é”™åˆ™ç›´æ¥å…¨é‡å…¥åº“
            df_with_id.drop(temp_id_col).write.mode("append").saveAsTable(full_target_table)
            return 
        finally:
            gc.collect() # æ˜¾å¼è§¦å‘ Python åƒåœ¾å›æ”¶
            print(f"ğŸ”“ Batch {batch_id}: Released Lock.")

    # --- æ•°æ®åˆ†æµä¸å…¥åº“ (é”å¤–æ‰§è¡Œä»¥æé«˜ IO å¹¶è¡Œåº¦) ---
    try:
        if result and result.success:
            print(f"âœ… Batch {batch_id}: {table_name} Validation Passed.")
            df_with_id.drop(temp_id_col).write.mode("append") \
                .option("mergeSchema", "true").saveAsTable(full_target_table)
        elif result:
            # æ”¶é›†é”™è¯¯è¡Œ ID å’Œè§„åˆ™
            errors = []
            for r in result.results:
                if not r.success:
                    col = r.expectation_config.kwargs.get("column", "Table")
                    rule = r.expectation_config.type
                    ids = r.result.get("unexpected_index_list")
                    if ids:
                        for row_id_dict in ids:
                            val = row_id_dict.get(temp_id_col)
                            if val is not None:
                                errors.append((val, f"[{col}] {rule}"))
            
            if not errors: # ç»“æœå¤±è´¥ä½†æ— å…·ä½“é”™è¯¯è¡Œï¼ˆè¡¨çº§è§„åˆ™å¤±è´¥ï¼‰
                df_with_id.drop(temp_id_col).write.mode("append").saveAsTable(full_target_table)
                return

            # åˆ›å»ºé”™è¯¯è¯¦æƒ…è¡¨
            error_schema = StructType([
                StructField(temp_id_col, LongType(), True),
                StructField("violated_rule", StringType(), True)
            ])
            error_info_df = spark_internal.createDataFrame(errors, schema=error_schema) \
                .groupBy(temp_id_col).agg(F.concat_ws("; ", F.collect_list("violated_rule")).alias("violated_rules"))

            bad_row_ids = [e[0] for e in errors]
            
            # åæ•°æ®åˆ†æµå…¥ Quarantine
            bad_df = df_with_id.filter(F.col(temp_id_col).isin(bad_row_ids)) \
                .join(error_info_df, on=temp_id_col, how="left") \
                .withColumn("raw_data", F.to_json(F.struct([c for c in df.columns]))) \
                .withColumn("origin_table", F.lit(table_name)) \
                .withColumn("ingestion_time", F.current_timestamp()) \
                .select(
                    F.col("origin_table").alias("table_name"),
                    F.lit(str(batch_id)).alias("gx_batch_id"),
                    "violated_rules", "raw_data", "ingestion_time"
                )
            bad_df.write.mode("append").option("mergeSchema", "true").saveAsTable(quarantine_table)
            
            # å¥½æ•°æ®åˆ†æµå…¥ Target
            good_df = df_with_id.filter(~F.col(temp_id_col).isin(bad_row_ids)).drop(temp_id_col)
            if good_df.limit(1).count() > 0:
                good_df.write.mode("append").option("mergeSchema", "true").saveAsTable(full_target_table)
            
            print(f"âš ï¸ Batch {batch_id}: Quarantined {len(set(bad_row_ids))} rows.")

    except Exception as e:
        print(f"âŒ Batch {batch_id} Write Error: {str(e)}")
        # æœ€åçš„å…œåº•ï¼šå¦‚æœå…¥åº“é€»è¾‘å´©æºƒï¼Œç¡®ä¿æ•°æ®è‡³å°‘ä¿å­˜åˆ°ç›®æ ‡è¡¨
        df_with_id.drop(temp_id_col).write.mode("append").saveAsTable(full_target_table)
    finally:
        # ã€å…³é”®ã€‘æ¸…ç† Spark å†…å­˜å ç”¨
        if df_with_id.is_cached:
            df_with_id.unpersist()
        
        # ä»…é’ˆå¯¹æœ¬æ‰¹æ¬¡åšå…ƒæ•°æ®æ¸…ç†ï¼Œæ¯” clearCache æ›´è½»é‡
        del result
        gc.collect()